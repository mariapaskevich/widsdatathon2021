Found `n_estimators` in params. Will use it instead of argument
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030521 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
Overriding the parameters from Reference Dataset.
categorical_column in param dict is overridden.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	auc's auc: 0.818621
Training until validation scores don't improve for 50 rounds
[2]	auc's auc: 0.822185
[3]	auc's auc: 0.827662
[4]	auc's auc: 0.829174
[5]	auc's auc: 0.831133
[6]	auc's auc: 0.832416
[7]	auc's auc: 0.833193
[8]	auc's auc: 0.834483
[9]	auc's auc: 0.835705
[10]	auc's auc: 0.83688
[11]	auc's auc: 0.837639
[12]	auc's auc: 0.838215
[13]	auc's auc: 0.83891
[14]	auc's auc: 0.839456
[15]	auc's auc: 0.840531
[16]	auc's auc: 0.841364
[17]	auc's auc: 0.841921
[18]	auc's auc: 0.842655
[19]	auc's auc: 0.843266
[20]	auc's auc: 0.843738
[21]	auc's auc: 0.844397
[22]	auc's auc: 0.844868
[23]	auc's auc: 0.845354
[24]	auc's auc: 0.845705
[25]	auc's auc: 0.845982
[26]	auc's auc: 0.846508
[27]	auc's auc: 0.847063
[28]	auc's auc: 0.84759
[29]	auc's auc: 0.848172
[30]	auc's auc: 0.848559
[31]	auc's auc: 0.849002
[32]	auc's auc: 0.849553
[33]	auc's auc: 0.850007
[34]	auc's auc: 0.850414
[35]	auc's auc: 0.850695
[36]	auc's auc: 0.851046
[37]	auc's auc: 0.851375
[38]	auc's auc: 0.851687
[39]	auc's auc: 0.851911
[40]	auc's auc: 0.852126
[41]	auc's auc: 0.852439
[42]	auc's auc: 0.85282
[43]	auc's auc: 0.853053
[44]	auc's auc: 0.853481
[45]	auc's auc: 0.854014
[46]	auc's auc: 0.854202
[47]	auc's auc: 0.854528
[48]	auc's auc: 0.854836
[49]	auc's auc: 0.855041
[50]	auc's auc: 0.855382
[51]	auc's auc: 0.855726
[52]	auc's auc: 0.856246
[53]	auc's auc: 0.856405
[54]	auc's auc: 0.856574
[55]	auc's auc: 0.856872
[56]	auc's auc: 0.857109
[57]	auc's auc: 0.857345
[58]	auc's auc: 0.857559
[59]	auc's auc: 0.857685
[60]	auc's auc: 0.858002
[61]	auc's auc: 0.858183
[62]	auc's auc: 0.858449
[63]	auc's auc: 0.858675
[64]	auc's auc: 0.858804
[65]	auc's auc: 0.858973
[66]	auc's auc: 0.859167
[67]	auc's auc: 0.859385
[68]	auc's auc: 0.859638
[69]	auc's auc: 0.85983
[70]	auc's auc: 0.860108
[71]	auc's auc: 0.860231
[72]	auc's auc: 0.860526
[73]	auc's auc: 0.860899
[74]	auc's auc: 0.861131
[75]	auc's auc: 0.861218
[76]	auc's auc: 0.861356
[77]	auc's auc: 0.861431
[78]	auc's auc: 0.861648
[79]	auc's auc: 0.861805
[80]	auc's auc: 0.861952
[81]	auc's auc: 0.862215
[82]	auc's auc: 0.862382
[83]	auc's auc: 0.862607
[84]	auc's auc: 0.862731
[85]	auc's auc: 0.862903
[86]	auc's auc: 0.863054
[87]	auc's auc: 0.863205
[88]	auc's auc: 0.863314
[89]	auc's auc: 0.863386
[90]	auc's auc: 0.863513
[91]	auc's auc: 0.863658
[92]	auc's auc: 0.863807
[93]	auc's auc: 0.863816
[94]	auc's auc: 0.864087
[95]	auc's auc: 0.864127
[96]	auc's auc: 0.86438
[97]	auc's auc: 0.864463
[98]	auc's auc: 0.864559
[99]	auc's auc: 0.864679
[100]	auc's auc: 0.864858
[101]	auc's auc: 0.865004
[102]	auc's auc: 0.865213
[103]	auc's auc: 0.865261
[104]	auc's auc: 0.865551
[105]	auc's auc: 0.865743
[106]	auc's auc: 0.865811
[107]	auc's auc: 0.865878
[108]	auc's auc: 0.866118
[109]	auc's auc: 0.866185
[110]	auc's auc: 0.866445
[111]	auc's auc: 0.866471
[112]	auc's auc: 0.866477
[113]	auc's auc: 0.866619
[114]	auc's auc: 0.86667
[115]	auc's auc: 0.866849
[116]	auc's auc: 0.866931
[117]	auc's auc: 0.866982
[118]	auc's auc: 0.867052
[119]	auc's auc: 0.867154
[120]	auc's auc: 0.867179
[121]	auc's auc: 0.867294
[122]	auc's auc: 0.867307
[123]	auc's auc: 0.867378
[124]	auc's auc: 0.867571
[125]	auc's auc: 0.867661
[126]	auc's auc: 0.867729
[127]	auc's auc: 0.867809
[128]	auc's auc: 0.867914
[129]	auc's auc: 0.86799
[130]	auc's auc: 0.868007
[131]	auc's auc: 0.868151
[132]	auc's auc: 0.868191
[133]	auc's auc: 0.868349
[134]	auc's auc: 0.868366
[135]	auc's auc: 0.868379
[136]	auc's auc: 0.868444
[137]	auc's auc: 0.868567
[138]	auc's auc: 0.86865
[139]	auc's auc: 0.868681
[140]	auc's auc: 0.868733
[141]	auc's auc: 0.868802
[142]	auc's auc: 0.86881
[143]	auc's auc: 0.868827
[144]	auc's auc: 0.868854
[145]	auc's auc: 0.868897
[146]	auc's auc: 0.868945
[147]	auc's auc: 0.868995
[148]	auc's auc: 0.868999
[149]	auc's auc: 0.869021
[150]	auc's auc: 0.86903
[151]	auc's auc: 0.869084
[152]	auc's auc: 0.869215
[153]	auc's auc: 0.86917
[154]	auc's auc: 0.869198
[155]	auc's auc: 0.86918
[156]	auc's auc: 0.869209
[157]	auc's auc: 0.869267
[158]	auc's auc: 0.86928
[159]	auc's auc: 0.869273
[160]	auc's auc: 0.869272
[161]	auc's auc: 0.869306
[162]	auc's auc: 0.869323
[163]	auc's auc: 0.86938
[164]	auc's auc: 0.869402
[165]	auc's auc: 0.869393
[166]	auc's auc: 0.869362
[167]	auc's auc: 0.86937
[168]	auc's auc: 0.869413
[169]	auc's auc: 0.869448
[170]	auc's auc: 0.869448
[171]	auc's auc: 0.869495
[172]	auc's auc: 0.869522
[173]	auc's auc: 0.869583
[174]	auc's auc: 0.869613
[175]	auc's auc: 0.869606
[176]	auc's auc: 0.869609
[177]	auc's auc: 0.869646
[178]	auc's auc: 0.869665
[179]	auc's auc: 0.86964
[180]	auc's auc: 0.869626
[181]	auc's auc: 0.869626
[182]	auc's auc: 0.869605
[183]	auc's auc: 0.869742
[184]	auc's auc: 0.869766
[185]	auc's auc: 0.86977
[186]	auc's auc: 0.869793
[187]	auc's auc: 0.869821
[188]	auc's auc: 0.869781
[189]	auc's auc: 0.869778
[190]	auc's auc: 0.869772
[191]	auc's auc: 0.869789
[192]	auc's auc: 0.869762
[193]	auc's auc: 0.869762
[194]	auc's auc: 0.869765
[195]	auc's auc: 0.869775
[196]	auc's auc: 0.869774
[197]	auc's auc: 0.869806
[198]	auc's auc: 0.869819
[199]	auc's auc: 0.869864
[200]	auc's auc: 0.869852
[201]	auc's auc: 0.869854
[202]	auc's auc: 0.869868
[203]	auc's auc: 0.869864
[204]	auc's auc: 0.869841
[205]	auc's auc: 0.869819
[206]	auc's auc: 0.869834
[207]	auc's auc: 0.869857
[208]	auc's auc: 0.869883
[209]	auc's auc: 0.869889
[210]	auc's auc: 0.86994
[211]	auc's auc: 0.869967
[212]	auc's auc: 0.86997
[213]	auc's auc: 0.869947
[214]	auc's auc: 0.869969
[215]	auc's auc: 0.869999
[216]	auc's auc: 0.870003
[217]	auc's auc: 0.870061
[218]	auc's auc: 0.870031
[219]	auc's auc: 0.870024
[220]	auc's auc: 0.870044
[221]	auc's auc: 0.870055
[222]	auc's auc: 0.870026
[223]	auc's auc: 0.870081
[224]	auc's auc: 0.870113
[225]	auc's auc: 0.87013
[226]	auc's auc: 0.870155
[227]	auc's auc: 0.870153
[228]	auc's auc: 0.870165
[229]	auc's auc: 0.870182
[230]	auc's auc: 0.870207
[231]	auc's auc: 0.870178
[232]	auc's auc: 0.870228
[233]	auc's auc: 0.870238
[234]	auc's auc: 0.87026
[235]	auc's auc: 0.870284
[236]	auc's auc: 0.870297
[237]	auc's auc: 0.87031
[238]	auc's auc: 0.870306
[239]	auc's auc: 0.870308
[240]	auc's auc: 0.870411
[241]	auc's auc: 0.870396
[242]	auc's auc: 0.870444
[243]	auc's auc: 0.870441
[244]	auc's auc: 0.870472
[245]	auc's auc: 0.870533
[246]	auc's auc: 0.870511
[247]	auc's auc: 0.870514
[248]	auc's auc: 0.870523
[249]	auc's auc: 0.870482
[250]	auc's auc: 0.870424
[251]	auc's auc: 0.870384
[252]	auc's auc: 0.870388
[253]	auc's auc: 0.87038
[254]	auc's auc: 0.870402
[255]	auc's auc: 0.870394
[256]	auc's auc: 0.870434
[257]	auc's auc: 0.870437
[258]	auc's auc: 0.870412
[259]	auc's auc: 0.870422
[260]	auc's auc: 0.870425
[261]	auc's auc: 0.870418
[262]	auc's auc: 0.870393
[263]	auc's auc: 0.870412
[264]	auc's auc: 0.870427
[265]	auc's auc: 0.870438
[266]	auc's auc: 0.870438
[267]	auc's auc: 0.870429
[268]	auc's auc: 0.870452
[269]	auc's auc: 0.870447
[270]	auc's auc: 0.870485
[271]	auc's auc: 0.870452
[272]	auc's auc: 0.870466
[273]	auc's auc: 0.870465
[274]	auc's auc: 0.870466
[275]	auc's auc: 0.870447
[276]	auc's auc: 0.870472
[277]	auc's auc: 0.870455
[278]	auc's auc: 0.870479
[279]	auc's auc: 0.870462
[280]	auc's auc: 0.870479
[281]	auc's auc: 0.870522
[282]	auc's auc: 0.870584
[283]	auc's auc: 0.870561
[284]	auc's auc: 0.87054
[285]	auc's auc: 0.870529
[286]	auc's auc: 0.870488
[287]	auc's auc: 0.870494
[288]	auc's auc: 0.870478
[289]	auc's auc: 0.870496
[290]	auc's auc: 0.870464
[291]	auc's auc: 0.870472
[292]	auc's auc: 0.870461
[293]	auc's auc: 0.870485
[294]	auc's auc: 0.870493
[295]	auc's auc: 0.870515
[296]	auc's auc: 0.870525
[297]	auc's auc: 0.870502
[298]	auc's auc: 0.870476
[299]	auc's auc: 0.8705
[300]	auc's auc: 0.870507
[301]	auc's auc: 0.870521
[302]	auc's auc: 0.870528
[303]	auc's auc: 0.870519
[304]	auc's auc: 0.870511
[305]	auc's auc: 0.870549
[306]	auc's auc: 0.870528
[307]	auc's auc: 0.870543
[308]	auc's auc: 0.870523
[309]	auc's auc: 0.8705
[310]	auc's auc: 0.870489
[311]	auc's auc: 0.870493
[312]	auc's auc: 0.870493
[313]	auc's auc: 0.870519
[314]	auc's auc: 0.870559
[315]	auc's auc: 0.870549
[316]	auc's auc: 0.870549
[317]	auc's auc: 0.870597
[318]	auc's auc: 0.870581
[319]	auc's auc: 0.870577
[320]	auc's auc: 0.870634
[321]	auc's auc: 0.870658
[322]	auc's auc: 0.870687
[323]	auc's auc: 0.870674
[324]	auc's auc: 0.870668
[325]	auc's auc: 0.870682
[326]	auc's auc: 0.870703
[327]	auc's auc: 0.870684
[328]	auc's auc: 0.870702
[329]	auc's auc: 0.870712
[330]	auc's auc: 0.870701
[331]	auc's auc: 0.870678
[332]	auc's auc: 0.870673
[333]	auc's auc: 0.870705
[334]	auc's auc: 0.870708
[335]	auc's auc: 0.870686
[336]	auc's auc: 0.870674
[337]	auc's auc: 0.870664
[338]	auc's auc: 0.8707
[339]	auc's auc: 0.870715
[340]	auc's auc: 0.870719
[341]	auc's auc: 0.870765
[342]	auc's auc: 0.870756
[343]	auc's auc: 0.87073
[344]	auc's auc: 0.87073
[345]	auc's auc: 0.8707
[346]	auc's auc: 0.870677
[347]	auc's auc: 0.870647
[348]	auc's auc: 0.870624
[349]	auc's auc: 0.87061
[350]	auc's auc: 0.870633
[351]	auc's auc: 0.870635
[352]	auc's auc: 0.870653
[353]	auc's auc: 0.870658
[354]	auc's auc: 0.870623
[355]	auc's auc: 0.870631
[356]	auc's auc: 0.870625
[357]	auc's auc: 0.870621
[358]	auc's auc: 0.870604
[359]	auc's auc: 0.870584
[360]	auc's auc: 0.870585
[361]	auc's auc: 0.870597
[362]	auc's auc: 0.87062
[363]	auc's auc: 0.870599
[364]	auc's auc: 0.870637
[365]	auc's auc: 0.870628
[366]	auc's auc: 0.870602
[367]	auc's auc: 0.870596
[368]	auc's auc: 0.870598
[369]	auc's auc: 0.870597
[370]	auc's auc: 0.870594
[371]	auc's auc: 0.870577
[372]	auc's auc: 0.870548
[373]	auc's auc: 0.87057
[374]	auc's auc: 0.870546
[375]	auc's auc: 0.870546
[376]	auc's auc: 0.870546
[377]	auc's auc: 0.870539
[378]	auc's auc: 0.870509
[379]	auc's auc: 0.870522
[380]	auc's auc: 0.870521
[381]	auc's auc: 0.870517
[382]	auc's auc: 0.870497
[383]	auc's auc: 0.870472
[384]	auc's auc: 0.87047
[385]	auc's auc: 0.870442
[386]	auc's auc: 0.870455
[387]	auc's auc: 0.870461
[388]	auc's auc: 0.870474
[389]	auc's auc: 0.870499
[390]	auc's auc: 0.870542
[391]	auc's auc: 0.870566
Early stopping, best iteration is:
[341]	auc's auc: 0.870765
0.8707646615410706
0.9814015587737335
0.8707646615410706
0.8591528058327417
Found `n_estimators` in params. Will use it instead of argument
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030378 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
Overriding the parameters from Reference Dataset.
categorical_column in param dict is overridden.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	val's auc: 0.816129
Training until validation scores don't improve for 50 rounds
[2]	val's auc: 0.820326
[3]	val's auc: 0.823413
[4]	val's auc: 0.82511
[5]	val's auc: 0.828994
[6]	val's auc: 0.830787
[7]	val's auc: 0.832868
[8]	val's auc: 0.833336
[9]	val's auc: 0.834719
[10]	val's auc: 0.836238
[11]	val's auc: 0.836676
[12]	val's auc: 0.837239
[13]	val's auc: 0.838033
[14]	val's auc: 0.838605
[15]	val's auc: 0.83932
[16]	val's auc: 0.839875
[17]	val's auc: 0.840939
[18]	val's auc: 0.841612
[19]	val's auc: 0.842442
[20]	val's auc: 0.842988
[21]	val's auc: 0.843537
[22]	val's auc: 0.84413
[23]	val's auc: 0.844623
[24]	val's auc: 0.845002
[25]	val's auc: 0.845402
[26]	val's auc: 0.845684
[27]	val's auc: 0.846303
[28]	val's auc: 0.846665
[29]	val's auc: 0.847147
[30]	val's auc: 0.84745
[31]	val's auc: 0.847951
[32]	val's auc: 0.848322
[33]	val's auc: 0.848855
[34]	val's auc: 0.849259
[35]	val's auc: 0.849832
[36]	val's auc: 0.850208
[37]	val's auc: 0.850602
[38]	val's auc: 0.850816
[39]	val's auc: 0.851311
[40]	val's auc: 0.851566
[41]	val's auc: 0.851996
[42]	val's auc: 0.852246
[43]	val's auc: 0.852606
[44]	val's auc: 0.852801
[45]	val's auc: 0.853072
[46]	val's auc: 0.853454
[47]	val's auc: 0.853634
[48]	val's auc: 0.853869
[49]	val's auc: 0.854237
[50]	val's auc: 0.854482
[51]	val's auc: 0.854786
[52]	val's auc: 0.854992
[53]	val's auc: 0.855091
[54]	val's auc: 0.855262
[55]	val's auc: 0.855593
[56]	val's auc: 0.855816
[57]	val's auc: 0.856048
[58]	val's auc: 0.856324
[59]	val's auc: 0.856565
[60]	val's auc: 0.856863
[61]	val's auc: 0.857093
[62]	val's auc: 0.857211
[63]	val's auc: 0.857548
[64]	val's auc: 0.857673
[65]	val's auc: 0.858059
[66]	val's auc: 0.858313
[67]	val's auc: 0.858515
[68]	val's auc: 0.858614
[69]	val's auc: 0.858763
[70]	val's auc: 0.859094
[71]	val's auc: 0.859254
[72]	val's auc: 0.859503
[73]	val's auc: 0.859501
[74]	val's auc: 0.859867
[75]	val's auc: 0.860116
[76]	val's auc: 0.860365
[77]	val's auc: 0.860615
[78]	val's auc: 0.860764
[79]	val's auc: 0.860909
[80]	val's auc: 0.861013
[81]	val's auc: 0.861158
[82]	val's auc: 0.8612
[83]	val's auc: 0.861333
[84]	val's auc: 0.861432
[85]	val's auc: 0.861657
[86]	val's auc: 0.861843
[87]	val's auc: 0.861933
[88]	val's auc: 0.861993
[89]	val's auc: 0.862115
[90]	val's auc: 0.862319
[91]	val's auc: 0.862331
[92]	val's auc: 0.862495
[93]	val's auc: 0.862834
[94]	val's auc: 0.863001
[95]	val's auc: 0.863117
[96]	val's auc: 0.863197
[97]	val's auc: 0.863257
[98]	val's auc: 0.863531
[99]	val's auc: 0.863737
[100]	val's auc: 0.863847
[101]	val's auc: 0.863921
[102]	val's auc: 0.864044
[103]	val's auc: 0.864138
[104]	val's auc: 0.864289
[105]	val's auc: 0.864353
[106]	val's auc: 0.864443
[107]	val's auc: 0.864783
[108]	val's auc: 0.864984
[109]	val's auc: 0.865055
[110]	val's auc: 0.865142
[111]	val's auc: 0.865401
[112]	val's auc: 0.865448
[113]	val's auc: 0.865472
[114]	val's auc: 0.865679
[115]	val's auc: 0.865768
[116]	val's auc: 0.865851
[117]	val's auc: 0.865891
[118]	val's auc: 0.865972
[119]	val's auc: 0.866133
[120]	val's auc: 0.866348
[121]	val's auc: 0.866382
[122]	val's auc: 0.866586
[123]	val's auc: 0.866644
[124]	val's auc: 0.866707
[125]	val's auc: 0.866703
[126]	val's auc: 0.866844
[127]	val's auc: 0.866858
[128]	val's auc: 0.866941
[129]	val's auc: 0.866957
[130]	val's auc: 0.867034
[131]	val's auc: 0.867104
[132]	val's auc: 0.867122
[133]	val's auc: 0.867205
[134]	val's auc: 0.867268
[135]	val's auc: 0.867249
[136]	val's auc: 0.867316
[137]	val's auc: 0.867332
[138]	val's auc: 0.867417
[139]	val's auc: 0.867394
[140]	val's auc: 0.867482
[141]	val's auc: 0.867502
[142]	val's auc: 0.867526
[143]	val's auc: 0.867546
[144]	val's auc: 0.867588
[145]	val's auc: 0.867677
[146]	val's auc: 0.867695
[147]	val's auc: 0.867721
[148]	val's auc: 0.867753
[149]	val's auc: 0.867773
[150]	val's auc: 0.867814
[151]	val's auc: 0.867818
[152]	val's auc: 0.867794
[153]	val's auc: 0.867845
[154]	val's auc: 0.867911
[155]	val's auc: 0.867941
[156]	val's auc: 0.868036
[157]	val's auc: 0.868029
[158]	val's auc: 0.868068
[159]	val's auc: 0.868033
[160]	val's auc: 0.868023
[161]	val's auc: 0.867981
[162]	val's auc: 0.868071
[163]	val's auc: 0.868092
[164]	val's auc: 0.868171
[165]	val's auc: 0.868192
[166]	val's auc: 0.868226
[167]	val's auc: 0.86827
[168]	val's auc: 0.868277
[169]	val's auc: 0.868301
[170]	val's auc: 0.868456
[171]	val's auc: 0.868438
[172]	val's auc: 0.868491
[173]	val's auc: 0.868527
[174]	val's auc: 0.868575
[175]	val's auc: 0.868586
[176]	val's auc: 0.868566
[177]	val's auc: 0.868588
[178]	val's auc: 0.868591
[179]	val's auc: 0.868633
[180]	val's auc: 0.868627
[181]	val's auc: 0.868619
[182]	val's auc: 0.868643
[183]	val's auc: 0.868624
[184]	val's auc: 0.868606
[185]	val's auc: 0.868638
[186]	val's auc: 0.868629
[187]	val's auc: 0.868674
[188]	val's auc: 0.868703
[189]	val's auc: 0.868715
[190]	val's auc: 0.868738
[191]	val's auc: 0.868787
[192]	val's auc: 0.868756
[193]	val's auc: 0.868795
[194]	val's auc: 0.868773
[195]	val's auc: 0.868747
[196]	val's auc: 0.868883
[197]	val's auc: 0.868876
[198]	val's auc: 0.868899
[199]	val's auc: 0.868918
[200]	val's auc: 0.868917
[201]	val's auc: 0.868943
[202]	val's auc: 0.868926
[203]	val's auc: 0.868905
[204]	val's auc: 0.868892
[205]	val's auc: 0.869022
[206]	val's auc: 0.869013
[207]	val's auc: 0.868952
[208]	val's auc: 0.868952
[209]	val's auc: 0.868952
[210]	val's auc: 0.86902
[211]	val's auc: 0.869035
[212]	val's auc: 0.869067
[213]	val's auc: 0.869047
[214]	val's auc: 0.869048
[215]	val's auc: 0.869028
[216]	val's auc: 0.869013
[217]	val's auc: 0.869026
[218]	val's auc: 0.869044
[219]	val's auc: 0.869017
[220]	val's auc: 0.869021
[221]	val's auc: 0.869033
[222]	val's auc: 0.869052
[223]	val's auc: 0.869054
[224]	val's auc: 0.869064
[225]	val's auc: 0.869039
[226]	val's auc: 0.869026
[227]	val's auc: 0.868992
[228]	val's auc: 0.868978
[229]	val's auc: 0.868987
[230]	val's auc: 0.86903
[231]	val's auc: 0.869025
[232]	val's auc: 0.869046
[233]	val's auc: 0.869054
[234]	val's auc: 0.869058
[235]	val's auc: 0.869059
[236]	val's auc: 0.869019
[237]	val's auc: 0.869032
[238]	val's auc: 0.86902
[239]	val's auc: 0.869
[240]	val's auc: 0.868994
[241]	val's auc: 0.869
[242]	val's auc: 0.86898
[243]	val's auc: 0.869014
[244]	val's auc: 0.869011
[245]	val's auc: 0.86901
[246]	val's auc: 0.86897
[247]	val's auc: 0.868943
[248]	val's auc: 0.868931
[249]	val's auc: 0.868922
[250]	val's auc: 0.868904
[251]	val's auc: 0.868842
[252]	val's auc: 0.868854
[253]	val's auc: 0.868873
[254]	val's auc: 0.868909
[255]	val's auc: 0.868903
[256]	val's auc: 0.86892
[257]	val's auc: 0.86894
[258]	val's auc: 0.868931
[259]	val's auc: 0.868913
[260]	val's auc: 0.868938
[261]	val's auc: 0.868975
[262]	val's auc: 0.869013
Early stopping, best iteration is:
[212]	val's auc: 0.869067
0.959721546117169
0.8707646615410706
0.8591528058327417
Found `n_estimators` in params. Will use it instead of argument
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031135 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
Overriding the parameters from Reference Dataset.
categorical_column in param dict is overridden.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	val's auc: 0.819502
Training until validation scores don't improve for 50 rounds
[2]	val's auc: 0.826531
[3]	val's auc: 0.831971
[4]	val's auc: 0.833927
[5]	val's auc: 0.835381
[6]	val's auc: 0.836973
[7]	val's auc: 0.837367
[8]	val's auc: 0.83832
[9]	val's auc: 0.839235
[10]	val's auc: 0.840104
[11]	val's auc: 0.84061
[12]	val's auc: 0.841223
[13]	val's auc: 0.842005
[14]	val's auc: 0.842383
[15]	val's auc: 0.842813
[16]	val's auc: 0.843164
[17]	val's auc: 0.843446
[18]	val's auc: 0.843757
[19]	val's auc: 0.84415
[20]	val's auc: 0.84491
[21]	val's auc: 0.845221
[22]	val's auc: 0.845799
[23]	val's auc: 0.846044
[24]	val's auc: 0.846349
[25]	val's auc: 0.8467
[26]	val's auc: 0.846887
[27]	val's auc: 0.847179
[28]	val's auc: 0.84749
[29]	val's auc: 0.847917
[30]	val's auc: 0.848241
[31]	val's auc: 0.848601
[32]	val's auc: 0.848859
[33]	val's auc: 0.849216
[34]	val's auc: 0.849689
[35]	val's auc: 0.850046
[36]	val's auc: 0.850637
[37]	val's auc: 0.850772
[38]	val's auc: 0.851282
[39]	val's auc: 0.851599
[40]	val's auc: 0.851983
[41]	val's auc: 0.852265
[42]	val's auc: 0.852365
[43]	val's auc: 0.852829
[44]	val's auc: 0.853104
[45]	val's auc: 0.85326
[46]	val's auc: 0.853535
[47]	val's auc: 0.85373
[48]	val's auc: 0.854076
[49]	val's auc: 0.854262
[50]	val's auc: 0.854554
[51]	val's auc: 0.854895
[52]	val's auc: 0.855236
[53]	val's auc: 0.855456
[54]	val's auc: 0.855629
[55]	val's auc: 0.855885
[56]	val's auc: 0.856107
[57]	val's auc: 0.856219
[58]	val's auc: 0.856437
[59]	val's auc: 0.856784
[60]	val's auc: 0.857039
[61]	val's auc: 0.8573
[62]	val's auc: 0.857799
[63]	val's auc: 0.858119
[64]	val's auc: 0.858381
[65]	val's auc: 0.858567
[66]	val's auc: 0.858913
[67]	val's auc: 0.859194
[68]	val's auc: 0.859332
[69]	val's auc: 0.85948
[70]	val's auc: 0.859618
[71]	val's auc: 0.859862
[72]	val's auc: 0.86021
[73]	val's auc: 0.860367
[74]	val's auc: 0.860532
[75]	val's auc: 0.86077
[76]	val's auc: 0.861005
[77]	val's auc: 0.861305
[78]	val's auc: 0.86141
[79]	val's auc: 0.861703
[80]	val's auc: 0.86184
[81]	val's auc: 0.862115
[82]	val's auc: 0.862208
[83]	val's auc: 0.862438
[84]	val's auc: 0.862504
[85]	val's auc: 0.862558
[86]	val's auc: 0.862783
[87]	val's auc: 0.862887
[88]	val's auc: 0.86292
[89]	val's auc: 0.863051
[90]	val's auc: 0.863212
[91]	val's auc: 0.863687
[92]	val's auc: 0.86382
[93]	val's auc: 0.863924
[94]	val's auc: 0.864003
[95]	val's auc: 0.86424
[96]	val's auc: 0.86434
[97]	val's auc: 0.864392
[98]	val's auc: 0.864578
[99]	val's auc: 0.86472
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034055 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	val's auc: 0.819502
Training until validation scores don't improve for 10 rounds
[2]	val's auc: 0.826531
[3]	val's auc: 0.831971
[4]	val's auc: 0.833927
[5]	val's auc: 0.835381
[6]	val's auc: 0.836973
[7]	val's auc: 0.837367
[8]	val's auc: 0.83832
[9]	val's auc: 0.839235
[10]	val's auc: 0.840104
[11]	val's auc: 0.84061
[12]	val's auc: 0.841223
[13]	val's auc: 0.842005
[14]	val's auc: 0.842383
[15]	val's auc: 0.842813
[16]	val's auc: 0.843164
[17]	val's auc: 0.843446
[18]	val's auc: 0.843757
[19]	val's auc: 0.84415
[20]	val's auc: 0.84491
[21]	val's auc: 0.845221
[22]	val's auc: 0.845799
[23]	val's auc: 0.846044
[24]	val's auc: 0.846349
[25]	val's auc: 0.8467
[26]	val's auc: 0.846887
[27]	val's auc: 0.847179
[28]	val's auc: 0.84749
[29]	val's auc: 0.847917
[30]	val's auc: 0.848241
[31]	val's auc: 0.848601
[32]	val's auc: 0.848859
[33]	val's auc: 0.849216
[34]	val's auc: 0.849689
[35]	val's auc: 0.850046
[36]	val's auc: 0.850637
[37]	val's auc: 0.850772
[38]	val's auc: 0.851282
[39]	val's auc: 0.851599
[40]	val's auc: 0.851983
[41]	val's auc: 0.852265
[42]	val's auc: 0.852365
[43]	val's auc: 0.852829
[44]	val's auc: 0.853104
[45]	val's auc: 0.85326
[46]	val's auc: 0.853535
[47]	val's auc: 0.85373
[48]	val's auc: 0.854076
[49]	val's auc: 0.854262
[50]	val's auc: 0.854554
[51]	val's auc: 0.854895
[52]	val's auc: 0.855236
[53]	val's auc: 0.855456
[54]	val's auc: 0.855629
[55]	val's auc: 0.855885
[56]	val's auc: 0.856107
[57]	val's auc: 0.856219
[58]	val's auc: 0.856437
[59]	val's auc: 0.856784
[60]	val's auc: 0.857039
[61]	val's auc: 0.8573
[62]	val's auc: 0.857799
[63]	val's auc: 0.858119
[64]	val's auc: 0.858381
[65]	val's auc: 0.858567
[66]	val's auc: 0.858913
[67]	val's auc: 0.859194
[68]	val's auc: 0.859332
[69]	val's auc: 0.85948
[70]	val's auc: 0.859618
[71]	val's auc: 0.859862
[72]	val's auc: 0.86021
[73]	val's auc: 0.860367
[74]	val's auc: 0.860532
[75]	val's auc: 0.86077
[76]	val's auc: 0.861005
[77]	val's auc: 0.861305
[78]	val's auc: 0.86141
[79]	val's auc: 0.861703
[80]	val's auc: 0.86184
[81]	val's auc: 0.862115
[82]	val's auc: 0.862208
[83]	val's auc: 0.862438
[84]	val's auc: 0.862504
[85]	val's auc: 0.862558
[86]	val's auc: 0.862783
[87]	val's auc: 0.862887
[88]	val's auc: 0.86292
[89]	val's auc: 0.863051
[90]	val's auc: 0.863212
[91]	val's auc: 0.863687
[92]	val's auc: 0.86382
[93]	val's auc: 0.863924
[94]	val's auc: 0.864003
[95]	val's auc: 0.86424
[96]	val's auc: 0.86434
[97]	val's auc: 0.864392
[98]	val's auc: 0.864578
[99]	val's auc: 0.86472
[100]	val's auc: 0.864789
[101]	val's auc: 0.86481
[102]	val's auc: 0.864894
[103]	val's auc: 0.864944
[104]	val's auc: 0.865048
[105]	val's auc: 0.865122
[106]	val's auc: 0.86523
[107]	val's auc: 0.865362
[108]	val's auc: 0.865452
[109]	val's auc: 0.865753
[110]	val's auc: 0.865815
[111]	val's auc: 0.865883
[112]	val's auc: 0.866079
[113]	val's auc: 0.866326
[114]	val's auc: 0.866377
[115]	val's auc: 0.866376
[116]	val's auc: 0.866599
[117]	val's auc: 0.866643
[118]	val's auc: 0.866707
[119]	val's auc: 0.866967
[120]	val's auc: 0.867008
[121]	val's auc: 0.867055
[122]	val's auc: 0.867111
[123]	val's auc: 0.867165
[124]	val's auc: 0.867249
[125]	val's auc: 0.867254
[126]	val's auc: 0.867433
[127]	val's auc: 0.867493
[128]	val's auc: 0.867624
[129]	val's auc: 0.867675
[130]	val's auc: 0.867703
[131]	val's auc: 0.867756
[132]	val's auc: 0.867963
[133]	val's auc: 0.868036
[134]	val's auc: 0.868033
[135]	val's auc: 0.868095
[136]	val's auc: 0.868161
[137]	val's auc: 0.868359
[138]	val's auc: 0.868373
[139]	val's auc: 0.868348
[140]	val's auc: 0.868387
[141]	val's auc: 0.868493
[142]	val's auc: 0.868481
[143]	val's auc: 0.86855
[144]	val's auc: 0.868602
[145]	val's auc: 0.868623
[146]	val's auc: 0.868688
[147]	val's auc: 0.868829
[148]	val's auc: 0.868867
[149]	val's auc: 0.868862
[150]	val's auc: 0.868871
[151]	val's auc: 0.868889
[152]	val's auc: 0.868943
[153]	val's auc: 0.869049
[154]	val's auc: 0.869223
[155]	val's auc: 0.869227
[156]	val's auc: 0.869246
[157]	val's auc: 0.86924
[158]	val's auc: 0.869267
[159]	val's auc: 0.869277
[160]	val's auc: 0.869268
[161]	val's auc: 0.869281
[162]	val's auc: 0.869305
[163]	val's auc: 0.869311
[164]	val's auc: 0.869393
[165]	val's auc: 0.869417
[166]	val's auc: 0.869485
[167]	val's auc: 0.869501
[168]	val's auc: 0.869482
[169]	val's auc: 0.869484
[170]	val's auc: 0.869558
[171]	val's auc: 0.869626
[172]	val's auc: 0.869626
[173]	val's auc: 0.869694
[174]	val's auc: 0.869707
[175]	val's auc: 0.869724
[176]	val's auc: 0.869825
[177]	val's auc: 0.869789
[178]	val's auc: 0.869781
[179]	val's auc: 0.86976
[180]	val's auc: 0.869823
[181]	val's auc: 0.869838
[182]	val's auc: 0.869897
[183]	val's auc: 0.869913
[184]	val's auc: 0.869895
[185]	val's auc: 0.869893
[186]	val's auc: 0.869914
[187]	val's auc: 0.869931
[188]	val's auc: 0.869896
[189]	val's auc: 0.869917
[190]	val's auc: 0.869994
[191]	val's auc: 0.870014
[192]	val's auc: 0.869987
[193]	val's auc: 0.86999
[194]	val's auc: 0.869977
[195]	val's auc: 0.869983
[196]	val's auc: 0.869985
[197]	val's auc: 0.869957
[198]	val's auc: 0.86993
[199]	val's auc: 0.869909
[200]	val's auc: 0.869949
[201]	val's auc: 0.869951
Early stopping, best iteration is:
[191]	val's auc: 0.870014
0.9528829794389149
0.8690665334021913
0.8593699260266283
Found `n_estimators` in params. Will use it instead of argument
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030324 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
Overriding the parameters from Reference Dataset.
categorical_column in param dict is overridden.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	val's auc: 0.818621
Training until validation scores don't improve for 10 rounds
[2]	val's auc: 0.822185
[3]	val's auc: 0.827662
[4]	val's auc: 0.829174
[5]	val's auc: 0.831133
[6]	val's auc: 0.832416
[7]	val's auc: 0.833193
[8]	val's auc: 0.834483
[9]	val's auc: 0.835705
[10]	val's auc: 0.83688
[11]	val's auc: 0.837639
[12]	val's auc: 0.838215
[13]	val's auc: 0.83891
[14]	val's auc: 0.839456
[15]	val's auc: 0.840531
[16]	val's auc: 0.841364
[17]	val's auc: 0.841921
[18]	val's auc: 0.842655
[19]	val's auc: 0.843266
[20]	val's auc: 0.843738
[21]	val's auc: 0.844397
[22]	val's auc: 0.844868
[23]	val's auc: 0.845354
[24]	val's auc: 0.845705
[25]	val's auc: 0.845982
[26]	val's auc: 0.846508
[27]	val's auc: 0.847063
[28]	val's auc: 0.84759
[29]	val's auc: 0.848172
[30]	val's auc: 0.848559
[31]	val's auc: 0.849002
[32]	val's auc: 0.849553
[33]	val's auc: 0.850007
[34]	val's auc: 0.850414
[35]	val's auc: 0.850695
[36]	val's auc: 0.851046
[37]	val's auc: 0.851375
[38]	val's auc: 0.851687
[39]	val's auc: 0.851911
[40]	val's auc: 0.852126
[41]	val's auc: 0.852439
[42]	val's auc: 0.85282
[43]	val's auc: 0.853053
[44]	val's auc: 0.853481
[45]	val's auc: 0.854014
[46]	val's auc: 0.854202
[47]	val's auc: 0.854528
[48]	val's auc: 0.854836
[49]	val's auc: 0.855041
[50]	val's auc: 0.855382
[51]	val's auc: 0.855726
[52]	val's auc: 0.856246
[53]	val's auc: 0.856405
[54]	val's auc: 0.856574
[55]	val's auc: 0.856872
[56]	val's auc: 0.857109
[57]	val's auc: 0.857345
[58]	val's auc: 0.857559
[59]	val's auc: 0.857685
[60]	val's auc: 0.858002
[61]	val's auc: 0.858183
[62]	val's auc: 0.858449
[63]	val's auc: 0.858675
[64]	val's auc: 0.858804
[65]	val's auc: 0.858973
[66]	val's auc: 0.859167
[67]	val's auc: 0.859385
[68]	val's auc: 0.859638
[69]	val's auc: 0.85983
[70]	val's auc: 0.860108
[71]	val's auc: 0.860231
[72]	val's auc: 0.860526
[73]	val's auc: 0.860899
[74]	val's auc: 0.861131
[75]	val's auc: 0.861218
[76]	val's auc: 0.861356
[77]	val's auc: 0.861431
[78]	val's auc: 0.861648
[79]	val's auc: 0.861805
[80]	val's auc: 0.861952
[81]	val's auc: 0.862215
[82]	val's auc: 0.862382
[83]	val's auc: 0.862607
[84]	val's auc: 0.862731
[85]	val's auc: 0.862903
[86]	val's auc: 0.863054
[87]	val's auc: 0.863205
[88]	val's auc: 0.863314
[89]	val's auc: 0.863386
[90]	val's auc: 0.863513
[91]	val's auc: 0.863658
[92]	val's auc: 0.863807
[93]	val's auc: 0.863816
[94]	val's auc: 0.864087
[95]	val's auc: 0.864127
[96]	val's auc: 0.86438
[97]	val's auc: 0.864463
[98]	val's auc: 0.864559
[99]	val's auc: 0.864679
[100]	val's auc: 0.864858
[101]	val's auc: 0.865004
[102]	val's auc: 0.865213
[103]	val's auc: 0.865261
[104]	val's auc: 0.865551
[105]	val's auc: 0.865743
[106]	val's auc: 0.865811
[107]	val's auc: 0.865878
[108]	val's auc: 0.866118
[109]	val's auc: 0.866185
[110]	val's auc: 0.866445
[111]	val's auc: 0.866471
[112]	val's auc: 0.866477
[113]	val's auc: 0.866619
[114]	val's auc: 0.86667
[115]	val's auc: 0.866849
[116]	val's auc: 0.866931
[117]	val's auc: 0.866982
[118]	val's auc: 0.867052
[119]	val's auc: 0.867154
[120]	val's auc: 0.867179
[121]	val's auc: 0.867294
[122]	val's auc: 0.867307
[123]	val's auc: 0.867378
[124]	val's auc: 0.867571
[125]	val's auc: 0.867661
[126]	val's auc: 0.867729
[127]	val's auc: 0.867809
[128]	val's auc: 0.867914
[129]	val's auc: 0.86799
[130]	val's auc: 0.868007
[131]	val's auc: 0.868151
[132]	val's auc: 0.868191
[133]	val's auc: 0.868349
[134]	val's auc: 0.868366
[135]	val's auc: 0.868379
[136]	val's auc: 0.868444
[137]	val's auc: 0.868567
[138]	val's auc: 0.86865
[139]	val's auc: 0.868681
[140]	val's auc: 0.868733
[141]	val's auc: 0.868802
[142]	val's auc: 0.86881
[143]	val's auc: 0.868827
[144]	val's auc: 0.868854
[145]	val's auc: 0.868897
[146]	val's auc: 0.868945
[147]	val's auc: 0.868995
[148]	val's auc: 0.868999
[149]	val's auc: 0.869021
[150]	val's auc: 0.86903
[151]	val's auc: 0.869084
[152]	val's auc: 0.869215
[153]	val's auc: 0.86917
[154]	val's auc: 0.869198
[155]	val's auc: 0.86918
[156]	val's auc: 0.869209
[157]	val's auc: 0.869267
[158]	val's auc: 0.86928
[159]	val's auc: 0.869273
[160]	val's auc: 0.869272
[161]	val's auc: 0.869306
[162]	val's auc: 0.869323
[163]	val's auc: 0.86938
[164]	val's auc: 0.869402
[165]	val's auc: 0.869393
[166]	val's auc: 0.869362
[167]	val's auc: 0.86937
[168]	val's auc: 0.869413
[169]	val's auc: 0.869448
[170]	val's auc: 0.869448
[171]	val's auc: 0.869495
[172]	val's auc: 0.869522
[173]	val's auc: 0.869583
[174]	val's auc: 0.869613
[175]	val's auc: 0.869606
[176]	val's auc: 0.869609
[177]	val's auc: 0.869646
[178]	val's auc: 0.869665
[179]	val's auc: 0.86964
[180]	val's auc: 0.869626
[181]	val's auc: 0.869626
[182]	val's auc: 0.869605
[183]	val's auc: 0.869742
[184]	val's auc: 0.869766
[185]	val's auc: 0.86977
[186]	val's auc: 0.869793
[187]	val's auc: 0.869821
[188]	val's auc: 0.869781
[189]	val's auc: 0.869778
[190]	val's auc: 0.869772
[191]	val's auc: 0.869789
[192]	val's auc: 0.869762
[193]	val's auc: 0.869762
[194]	val's auc: 0.869765
[195]	val's auc: 0.869775
[196]	val's auc: 0.869774
[197]	val's auc: 0.869806
Early stopping, best iteration is:
[187]	val's auc: 0.869821
0.9532939839168344
0.8700139225127136
0.8622758507763351
Found `n_estimators` in params. Will use it instead of argument
[LightGBM] [Info] Number of positive: 18173, number of negative: 65827
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030717 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 25512
[LightGBM] [Info] Number of data points in the train set: 84000, number of used features: 177
Overriding the parameters from Reference Dataset.
categorical_column in param dict is overridden.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.216345 -> initscore=-1.287093
[LightGBM] [Info] Start training from score -1.287093
[1]	val's auc: 0.819502
Training until validation scores don't improve for 10 rounds
[2]	val's auc: 0.826531
[3]	val's auc: 0.831971
[4]	val's auc: 0.833927
[5]	val's auc: 0.835381
[6]	val's auc: 0.836973
[7]	val's auc: 0.837367
[8]	val's auc: 0.83832
[9]	val's auc: 0.839235
[10]	val's auc: 0.840104
[11]	val's auc: 0.84061
[12]	val's auc: 0.841223
[13]	val's auc: 0.842005
[14]	val's auc: 0.842383
[15]	val's auc: 0.842813
[16]	val's auc: 0.843164
[17]	val's auc: 0.843446
[18]	val's auc: 0.843757
[19]	val's auc: 0.84415
[20]	val's auc: 0.84491
[21]	val's auc: 0.845221
[22]	val's auc: 0.845799
[23]	val's auc: 0.846044
[24]	val's auc: 0.846349
[25]	val's auc: 0.8467
[26]	val's auc: 0.846887
[27]	val's auc: 0.847179
[28]	val's auc: 0.84749
[29]	val's auc: 0.847917
[30]	val's auc: 0.848241
[31]	val's auc: 0.848601
[32]	val's auc: 0.848859
[33]	val's auc: 0.849216
[34]	val's auc: 0.849689
[35]	val's auc: 0.850046
[36]	val's auc: 0.850637
[37]	val's auc: 0.850772
[38]	val's auc: 0.851282
[39]	val's auc: 0.851599
[40]	val's auc: 0.851983
[41]	val's auc: 0.852265
[42]	val's auc: 0.852365
[43]	val's auc: 0.852829
[44]	val's auc: 0.853104
[45]	val's auc: 0.85326
[46]	val's auc: 0.853535
[47]	val's auc: 0.85373
[48]	val's auc: 0.854076
[49]	val's auc: 0.854262
[50]	val's auc: 0.854554
[51]	val's auc: 0.854895
[52]	val's auc: 0.855236
[53]	val's auc: 0.855456
[54]	val's auc: 0.855629
[55]	val's auc: 0.855885
[56]	val's auc: 0.856107
[57]	val's auc: 0.856219
[58]	val's auc: 0.856437
[59]	val's auc: 0.856784
[60]	val's auc: 0.857039
[61]	val's auc: 0.8573
[62]	val's auc: 0.857799
[63]	val's auc: 0.858119
[64]	val's auc: 0.858381
[65]	val's auc: 0.858567
[66]	val's auc: 0.858913
[67]	val's auc: 0.859194
[68]	val's auc: 0.859332
[69]	val's auc: 0.85948
[70]	val's auc: 0.859618
[71]	val's auc: 0.859862
[72]	val's auc: 0.86021
[73]	val's auc: 0.860367
[74]	val's auc: 0.860532
[75]	val's auc: 0.86077
[76]	val's auc: 0.861005
[77]	val's auc: 0.861305
[78]	val's auc: 0.86141
[79]	val's auc: 0.861703
[80]	val's auc: 0.86184
[81]	val's auc: 0.862115
[82]	val's auc: 0.862208
[83]	val's auc: 0.862438
[84]	val's auc: 0.862504
[85]	val's auc: 0.862558
[86]	val's auc: 0.862783
[87]	val's auc: 0.862887
[88]	val's auc: 0.86292
[89]	val's auc: 0.863051
[90]	val's auc: 0.863212
